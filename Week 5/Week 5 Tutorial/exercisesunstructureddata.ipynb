{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Unstructured Data\n",
    "\n",
    "## EXERCISE 1: Working with Unstructured Data (SMS texts) - Feature Extraction\n",
    "\n",
    "\n",
    "[Adapted from http://radimrehurek.com/data_science_python/]\n",
    "\n",
    "Other references:\n",
    "* http://sebastianraschka.com/Articles/2014_naive_bayes_1.html\n",
    "* http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "* https://gist.github.com/zacstewart/5978000\n",
    "\n",
    "Other options:\n",
    "* http://scikit-learn.org/stable/datasets/#the-20-newsgroups-text-dataset\n",
    "* http://scikit-learn.org/stable/datasets/#rcv1-dataset\n",
    "\n",
    "### Download data from UCI ML data repo\n",
    "For the following exercise, we use some real-world SMS dataset which we dynamically download from the UCI ML data repository and then store in a local 'data' directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "DATA_URI = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "DATA_DIR = 'data'\n",
    "ARCHIVE_NAME = 'smsspamcollection.zip'\n",
    "FILE_NAME = 'SMSSpamCollection'\n",
    "\n",
    "# set up paths (in portable, OS-agnostic way)\n",
    "local_archive_path = os.path.join(DATA_DIR, ARCHIVE_NAME)\n",
    "local_file_path = os.path.join(DATA_DIR, FILE_NAME)\n",
    "\n",
    "## set up local data directory\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "## save file from DATA_URI to local_path\n",
    "urllib.request.urlretrieve(DATA_URI, local_archive_path)\n",
    "\n",
    "## extract content from archive\n",
    "z = zipfile.ZipFile(local_archive_path, 'r')\n",
    "z.extractall(DATA_DIR)\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and profile data using Pandas\n",
    "\n",
    "Pandas provides tools that streamline some of the data analysis and visualisation work we've done in previous exercises. \n",
    "\n",
    "[DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) is the most commonly used data structure in Pandas. It is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table.\n",
    "\n",
    "Let's use it now to read and profile our spam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "...    ...                                                ...\n",
      "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5570   ham               Will Ã¼ b going to esplanade fr home?\n",
      "5571   ham  Pity, * was in mood for that. So...any other s...\n",
      "5572   ham  The guy did some bitching but I acted like i'd...\n",
      "5573   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5574 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4827</td>\n",
       "      <td>4518</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4827   4518                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas\n",
    "\n",
    "messages = pandas.read_csv(local_file_path, sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages)\n",
    "\n",
    "# view aggregate statistics\n",
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text feature extraction\n",
    "\n",
    "- It's easy for humans to see the patterns that distinguish ham and spam messages. What kind of features could we feed to a machine learning algorithm?\n",
    "\n",
    "Feeding raw text data (e.g., \"Sorry, I'll call later\") to a machine learning algorithm would not be very useful. As a first step let's assume that there is a systematic difference between the words used for spam and ham.\n",
    "\n",
    "`scikit-learn` includes [several functions for creating feature vectors from text](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). These first tokenise -- split strings into words (e.g., \"Sorry\", \"I'll\", \"call\", \"later\").\n",
    "\n",
    "Then create feature vectors where indices correspond to a specific word and values correspond to the frequency of the corresponding word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape:\n",
      " (3734, 8745)\n",
      "Feature names:\n",
      " ['00', '000', '000pes', '008704050406', '0089', '0121', '01223585236', '01223585334', '0125698789', '02']\n",
      "\n",
      "Feature words for row 0:\n",
      " ['Ok']\n",
      "\n",
      "Original string for row0:\n",
      " Ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# First split in to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages.message, messages.label, test_size=0.33,\n",
    "                                                    random_state=5) # so we get the same results\n",
    "\n",
    "# Fit and transform training to feature vectors of words\n",
    "v = CountVectorizer(binary=True, lowercase=False)\n",
    "X_train_bin = v.fit_transform(X_train)\n",
    "print('Feature matrix shape:\\n', X_train_bin.shape)\n",
    "\n",
    "# scikit-learn vectorisers produce sparse array representations.\n",
    "# These store only observed features for each instance instead of a complete vector.\n",
    "# They're great for working with text data, but require a bit of work to inspect.\n",
    "\n",
    "# View feature names\n",
    "print('Feature names:\\n', v.get_feature_names()[:10])\n",
    "\n",
    "# View complete feature vector for row 0 as a list of words\n",
    "def iter_features(X, row, names):\n",
    "    for i in X[row].indices:\n",
    "        yield names[i]\n",
    "print('\\nFeature words for row 0:\\n', list(iter_features(X_train_bin, 0, v.get_feature_names())))\n",
    "\n",
    "# Compare to original data\n",
    "print('\\nOriginal string for row0:\\n', X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Feature Extraction Experimenting\n",
    "Now it is your turn:\n",
    " - check the extracted features of some other rows too\n",
    " - change the CountVectorizer() to one using term counts ('binary=False') and/or ngrams ('ngram_range=(1,2)'). The lecture slides give some tips on which configuration parameters are possible. How does this affect the shape of the document term matrix and the extracted features from the rows which you investigated before?\n",
    " \n",
    "Documentation of the CountVectoriser:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\\Original SMS string from row 10:\n",
      " For taking part in our mobile survey yesterday! You can now have 500 texts 2 use however you wish. 2 get txts just send TXT to 80160 T&C www.txt43.com 1.50p\n",
      "\n",
      "Feature words for row 10:\n",
      " ['you', 'get', 'in', 'to', 'can', 'www', 'For', 'taking', 'part', 'our', 'mobile', 'survey', 'yesterday', 'You', 'now', 'have', '500', 'texts', 'use', 'however', 'wish', 'txts', 'just', 'send', 'TXT', '80160', 'txt43', 'com', '50p']\n",
      "Note how the URL had been split into separate features and how single words or digits have been ignored.\n"
     ]
    }
   ],
   "source": [
    "print('n\\Original SMS string from row 10:\\n', X_train.iloc[10])\n",
    "print('\\nFeature words for row 10:\\n', list(iter_features(X_train_bin, 10, v.get_feature_names())))\n",
    "print('Note how the URL had been split into separate features and how single words or digits have been ignored.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE and watch next lecture video first. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2: SMS spam filtering with naive Bayes\n",
    "\n",
    "### Build pipelines and choose parameters\n",
    "\n",
    "`scikit-learn` includes pipeline functionality that makes it possible to specify and optimise a sequence of actions.\n",
    "\n",
    "Let's see whether the results in [Wang and Manning (2012)](http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) hold for the SMS spam data we're using here.\n",
    "\n",
    "Specifically we will compare multinomial naive Bayes to support vector machine with a degree-2 polynomial kernel. We will choose the best feature representation for both:\n",
    "* binary vs term frequency vs tfidf\n",
    "* unigrams vs bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MNB best params:\n",
      " {'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "SVM best params:\n",
      " {'tfidf__use_idf': False, 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "MNB test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.98      1586\n",
      "        spam       1.00      0.69      0.82       254\n",
      "\n",
      "    accuracy                           0.96      1840\n",
      "   macro avg       0.98      0.85      0.90      1840\n",
      "weighted avg       0.96      0.96      0.95      1840\n",
      "\n",
      "\n",
      "SVM test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1586\n",
      "        spam       0.98      0.75      0.85       254\n",
      "\n",
      "    accuracy                           0.96      1840\n",
      "   macro avg       0.97      0.88      0.92      1840\n",
      "weighted avg       0.96      0.96      0.96      1840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Pipeline for multinomial naive Bayes\n",
    "mnb = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultinomialNB())\n",
    "               ])\n",
    "\n",
    "# Pipeline for polynomial support vector machine\n",
    "svm = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVC(C=0.1, penalty='l2'))\n",
    "               ])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = [{'vect__binary': [True],\n",
    "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': [True, False]\n",
    "              },\n",
    "              {'vect__binary': [False],\n",
    "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': [True, False]\n",
    "              }\n",
    "             ]\n",
    "\n",
    "# Find best parameters for MNB and SVM\n",
    "gs_mnb = GridSearchCV(mnb, param_grid)\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "print('\\nMNB best params:\\n', gs_mnb.best_params_)\n",
    "\n",
    "gs_svm = GridSearchCV(svm, param_grid)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "print('\\nSVM best params:\\n', gs_svm.best_params_)\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nMNB test result:\\n', classification_report(y_test, gs_mnb.predict(X_test)))\n",
    "print('\\nSVM test result:\\n', classification_report(y_test, gs_svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Handling class imbalance with SVM\n",
    "\n",
    "- Which is better? Do we care about overall performance or just one of our classes? How does this compare to Wang and Manning's result?\n",
    "- The `fit()` method for `LinearSVC` includes the `class_weight` parameter with can help deal with imbalanced data. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`. Train a new SVM model with the best grid search parameters and `class_weight='balanced'`. Is this result better?\n",
    "- Is there a more appropriate scoring function we could pass to `GridSearchCV` (using `sklearn.metrics.make_scorer`)? Does this give different MNB or SVM results for the parameter grid above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM2 test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1586\n",
      "        spam       0.92      0.93      0.92       254\n",
      "\n",
      "    accuracy                           0.98      1840\n",
      "   macro avg       0.95      0.96      0.95      1840\n",
      "weighted avg       0.98      0.98      0.98      1840\n",
      "\n",
      "\n",
      "MNB GS2 best params:\n",
      " {'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "SVM GS2 best params:\n",
      " {'tfidf__use_idf': False, 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "MNB GS2 test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.98      1586\n",
      "        spam       1.00      0.69      0.82       254\n",
      "\n",
      "    accuracy                           0.96      1840\n",
      "   macro avg       0.98      0.85      0.90      1840\n",
      "weighted avg       0.96      0.96      0.95      1840\n",
      "\n",
      "\n",
      "SVM GS2 test result:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1586\n",
      "        spam       0.98      0.75      0.85       254\n",
      "\n",
      "    accuracy                           0.96      1840\n",
      "   macro avg       0.97      0.88      0.92      1840\n",
      "weighted avg       0.96      0.96      0.96      1840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - SVM gives a better result for spam, which is the category of interest.\n",
    "# Since it's the minority category in a binary problem, \n",
    "# we should be looking at precision and recall for spam only. \n",
    "\n",
    "# 2 - Spam recall and f-score are better while precision is worse. \n",
    "# Which to deploy depends on the appropriate precision/recall tradeoff\n",
    "svm2 = Pipeline([('vect', CountVectorizer(lowercase=False, binary=True, ngram_range=(1,1))),\n",
    "                 ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                 ('clf', LinearSVC(C=0.1, penalty='l2', class_weight='balanced'))\n",
    "                ])\n",
    "svm2.fit(X_train, y_train) \n",
    "print('\\nSVM2 test result:\\n', classification_report(y_test, svm2.predict(X_test)))\n",
    "\n",
    "# 3 - We can use `f1-score` with `pos_label='spam'` and `average='binary'`\n",
    "# This gives the same results for this parameter grid, but could be used to choose the best `class_weight`. \n",
    "# Note with fbeta_score, we could also specify whether we prefer precision or recall. \n",
    "\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label='spam', average='binary')\n",
    "gs_mnb2 = GridSearchCV(mnb, param_grid, scoring=f1_scorer)\n",
    "gs_mnb2.fit(X_train, y_train)\n",
    "print('\\nMNB GS2 best params:\\n', gs_mnb2.best_params_)\n",
    "\n",
    "gs_svm2 = GridSearchCV(svm, param_grid, scoring=f1_scorer)\n",
    "gs_svm2.fit(X_train, y_train)\n",
    "print('\\nSVM GS2 best params:\\n', gs_svm2.best_params_)\n",
    "\n",
    "print('\\nMNB GS2 test result:\\n', classification_report(y_test, gs_mnb2.predict(X_test)))\n",
    "print('\\nSVM GS2 test result:\\n', classification_report(y_test, gs_svm2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.* Please watch the lecture video about Text-based Forecasting first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 3: Forecasting movie gross with support vector regression\n",
    "\n",
    "Imagine you have an opportunity to invest in films after seeing descriptions. Perhaps you run a cinema and need to decide which films to show. If we could predict box office gross based on descriptions, then we'd have a good basis for investment decisions.\n",
    "\n",
    "In this exercise we'll predict movie gross with support vector regression. \n",
    "\n",
    "### Download reviews and movie gross data\n",
    "\n",
    "DBPedia converts Wikipedia pages into structured semantic web data. Let's use it to grab the data we need. DBpedia has a public SPARQL endpoint at http://dbpedia.org/sparql. Enter the following query and save as `CSV` under `Results Format` and click `Run Query`.\n",
    "\n",
    "#### Query\n",
    "```\n",
    "PREFIX category: <http://dbpedia.org/resource/Category:>\n",
    "PREFIX dbtype: <http://dbpedia.org/datatype/>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "SELECT ?dburl ?title ?budget ?gross ?abstract\n",
    "WHERE { \n",
    " ?dburl dcterms:subject category:2013_films .\n",
    "\n",
    " ?dburl foaf:name ?title .\n",
    " FILTER(LANG(?title) = \"en\") .\n",
    "\n",
    " ?dburl dbo:budget ?budget .\n",
    " FILTER(xsd:float(?budget) > xsd:float(\"1.0E7\")) .\n",
    " FILTER(datatype(?budget) = dbtype:usDollar) .\n",
    "\n",
    " ?dburl dbo:gross ?gross .\n",
    " FILTER(datatype(?gross) = dbtype:usDollar) .\n",
    "\n",
    " ?dburl dbo:abstract ?abstract .\n",
    " FILTER(LANG(?abstract) = \"en\") .\n",
    " FILTER(fn:string-length(?abstract) >= xsd:int(140)) .\n",
    "} \n",
    "```\n",
    "\n",
    "This will return the box office gross and the Wikipedia abstract for English films from 2013 that had a USD budget of more than 10 million. This will be our training data. Upload to your `data` directory on Jupyter Hub and rename to `movies.2013.csv`.\n",
    "\n",
    "Run the same query with `category:2014_films` to get test data. Upload to your `data` directory on Jupyter Hub and rename to `movies.2014.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\movies.2013.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1ef47a98a496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_movies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-1ef47a98a496>\u001b[0m in \u001b[0;36mread_movies\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gross'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\movies.2013.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "TRAIN_NAME = 'movies.2013.csv'\n",
    "TEST_NAME = 'movies.2014.csv'\n",
    "\n",
    "def read_movies(path):\n",
    "    data = []\n",
    "    target = []\n",
    "    for d in csv.DictReader(open(path)):\n",
    "        data.append(d['abstract'])\n",
    "        target.append(float(d['gross']))\n",
    "    return data, target\n",
    "\n",
    "train_path = os.path.join('data', TRAIN_NAME)\n",
    "X_train, y_train = read_movies(train_path)\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "test_path = os.path.join('data', TEST_NAME)\n",
    "X_test, y_test = read_movies(test_path)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select parameters for support vector regression\n",
    "\n",
    "Let's see whether the results in [Kogan et al. (2009)](http://www.cs.cmu.edu/~nasmith/papers/kogan+levin+routledge+sagi+smith.naacl09.pdf) hold for the movie gross data we're using here.\n",
    "\n",
    "Specifically we will choose the best feature representation for both:\n",
    "* binary vs term frequency vs tfidf\n",
    "* unigrams vs bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "240 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 499, in fit\n",
      "    sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1180, in _fit_liblinear\n",
      "    y_ind = np.asarray(y_ind, dtype=np.float64).ravel()\n",
      "ValueError: could not convert string to float: 'ham'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'ham'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a05d832c3212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Find best parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mgs_svr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mgs_svr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Grid search mean and stdev:\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    501\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;31m# LibLinear wants targets as doubles, even for classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m     \u001b[0my_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[0my_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequirements\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'ham'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "svr = Pipeline([('vect', CountVectorizer(lowercase=True)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVR(epsilon=0.1, max_iter=40000))\n",
    "               ])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vect__binary': [True, False],\n",
    "              'tfidf__use_idf': [True, False],\n",
    "              'tfidf__sublinear_tf': [True, False],\n",
    "              'clf__C': [1e8, 1e9, 1e10],\n",
    "             }\n",
    "\n",
    "# Find best parameters\n",
    "gs_svr = GridSearchCV(svr, param_grid)\n",
    "gs_svr.fit(X_train, y_train)\n",
    "print('Grid search mean and stdev:\\n')\n",
    "\n",
    "scoring = gs_svr.cv_results_\n",
    "for mean_score, std, params in zip(scoring['mean_test_score'],scoring['std_test_score'],scoring['params']):\n",
    "    print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    "            mean_score, std * 2, params))\n",
    "    \n",
    "\n",
    "print('\\nSVR best params:\\n', gs_svr.best_params_)\n",
    "print('\\nSVR r-squared on training data:\\n', gs_svr.score(X_train, y_train))\n",
    "print('\\nSVR r-squared on test data:\\n', gs_svr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Evaluation and discussion\n",
    "\n",
    "- According to the 95% prediction interval, how close will our predictions be to the actual value? What if we calculate over the test data instead?\n",
    "- Draw a residual plot with training data in blue and test data in green. Is a linear model appropriate for our data? Is prediction performance comparable to training performance?\n",
    "- How could we improve this experimental setup? We can use `gs_svr.steps` to access feature names (`vect.get_feature_names()`) and weights (`clf.coef_`) from the pipeline components respectively. Use Python `zip` function to combine and sort these. What do the highest-weighted features tell us about our experimental setup?\n",
    "- Would you use this model to pick investments?\n",
    "- Maybe we're not predicting the right thing. What derived values could we predict? If you have time to try them, are they better?\n",
    "- Is there other text data that might be a better predictor of box office returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 1 - \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "se = math.sqrt(mean_squared_error(y_train, gs_svr.predict(X_train)))\n",
    "print('Standard error:', se)\n",
    "print('\\nPredictions should be within {:.1f}M dollars at 95% confidence according to training data.'.format(2*se/1e6))\n",
    "se_test = math.sqrt(mean_squared_error(y_test, gs_svr.predict(X_test)))\n",
    "print('Predictions should be within {:.1f}M dollars at 95% confidence according to test data.'.format(2*se_test/1e6))\n",
    "\n",
    "# 2 - Training residuals are nearly zero for most movies with a few exceptions.\n",
    "#     This may indicate overfitting. The residual plot for test data is OK.\n",
    "import matplotlib.pyplot as plt\n",
    "print('\\nResidual plot for training data (blue) and test data (green) below')\n",
    "_ = plt.scatter(gs_svr.predict(X_train), y_train-gs_svr.predict(X_train), c='blue', s=40, alpha=0.5, edgecolor='white')\n",
    "_ = plt.scatter(gs_svr.predict(X_test), y_test-gs_svr.predict(X_test), c='green', s=40, alpha=0.5, edgecolor='white')\n",
    "_ = plt.plot([-10,50], [0,0], c='black')\n",
    "_ = plt.ylabel('Residuals ($y - \\hat{y}$)')\n",
    "_ = plt.xlabel('Predicted values ($\\hat{y}$)')\n",
    "\n",
    "# 3 - More data!!!\n",
    "#     It would be better to have another chunk of data, e.g., from 2012.\n",
    "#     Then we could train on 2012 and tune our parameters on 2013.\n",
    "#     2014 could then serve as a final held-out test set to evaluate generalisation.\n",
    "#     It may be better to predict return / budget instead of the raw return value.\n",
    "#     We should also compare to a baseline model, e.g., using budget to predict gross return.\n",
    "#     Wikipedia is a bad source of descriptions for our scenario, since it is constantly\n",
    "#     edited and so we have data leakage. We could use the version of the Wikipedia\n",
    "#     abstract from when the movie was released.\n",
    "#     Printing the highest-weighted features highlights issues of data leakage..\n",
    "steps = dict(gs_svr.best_estimator_.steps) # access pipeline components directly\n",
    "vect = steps['vect'] # vectorizer component\n",
    "clf = steps['clf'] # classifier component\n",
    "feature_weights = clf.coef_\n",
    "feature_names = vect.get_feature_names()\n",
    "print('\\nTop 20 features:')\n",
    "sorted(zip(feature_weights, feature_names), reverse=True)[:20] # print top 20 features\n",
    "\n",
    "# 4 - No way would I deploy. The data leakage here means we can't trust this experiment setup.\n",
    "\n",
    "# 5 - We could try the ratio of gross to budget, then multiply a prediction by\n",
    "#     budget to get predicted gross.\n",
    "\n",
    "# 6 - Reviews, e.g., from Metacritic or Rotten Tomatoes.\n",
    "#     http://www.metacritic.com/feature/film-quality-vs-box-office-grosses\n",
    "#     Here it would be good to compare to a baseline that predicts gross based on star ratings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bea42e1b0e07028483ba0ff26b9b4dc4fa162e9d0ccb6b0507d54b9d42d30653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
