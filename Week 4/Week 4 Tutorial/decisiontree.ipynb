{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Decision trees\n",
    "\n",
    "\n",
    "\n",
    "Let's try a decision tree on Iris data.\n",
    "\n",
    "### Train and view a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "key=', '.join(['{}={}'.format(i,name) for i,name in enumerate(iris.target_names)])\n",
    "\n",
    "# First let's create a train and test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.33,\n",
    "                                                    random_state=5) # so we get the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report (0=setosa, 1=versicolor, 2=virginica):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.89      0.94      0.91        17\n",
      "           2       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.94        50\n",
      "   macro avg       0.94      0.94      0.94        50\n",
      "weighted avg       0.94      0.94      0.94        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Let's fit a model\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "_ = tree.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate\n",
    "print('Classification report ({}):\\n'.format(key))\n",
    "print(classification_report(Y_test, tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree:\n",
      "\n",
      "if ( petal length (cm) <= 2.449999988079071 ) {\n",
      "    return setosa ( 34 examples )\n",
      "}\n",
      "else {\n",
      "    if ( petal width (cm) <= 1.75 ) {\n",
      "        return versicolor ( 33 examples )\n",
      "        return virginica ( 3 examples )\n",
      "    }\n",
      "    else {\n",
      "        return virginica ( 30 examples )\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#From http://chrisstrelioff.ws/sandbox/2015/06/08/decision_trees_in_python_with_scikit_learn_and_pandas.html\n",
    "def get_code(tree, feature_names, target_names, spacer_base=\"    \"):\n",
    "    \"\"\"Produce psuedo-code for decision tree.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-leant DescisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    target_names -- list of target (class) names.\n",
    "    spacer_base -- used for spacing code (default: \"    \").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    based on http://stackoverflow.com/a/30104792.\n",
    "    \"\"\"\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    value = tree.tree_.value\n",
    "\n",
    "    def recurse(left, right, threshold, features, node, depth):\n",
    "        spacer = spacer_base * depth\n",
    "        if (threshold[node] != -2):\n",
    "            print(spacer + \"if ( \" + features[node] + \" <= \" + \\\n",
    "                  str(threshold[node]) + \" ) {\")\n",
    "            if left[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            left[node], depth+1)\n",
    "            print(spacer + \"}\\n\" + spacer +\"else {\")\n",
    "            if right[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            right[node], depth+1)\n",
    "            print(spacer + \"}\")\n",
    "        else:\n",
    "            target = value[node]\n",
    "            for i, v in zip(np.nonzero(target)[1],\n",
    "                            target[np.nonzero(target)]):\n",
    "                target_name = target_names[i]\n",
    "                target_count = int(v)\n",
    "                print(spacer + \"return \" + str(target_name) + \\\n",
    "                      \" ( \" + str(target_count) + \" examples )\")\n",
    "\n",
    "    recurse(left, right, threshold, features, 0, 0)\n",
    "    \n",
    "print('Decision tree:\\n')\n",
    "get_code(tree, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Model selection on test data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McNemar's test\n",
    "\n",
    "McNemar's test is [recommended when we have a single test split](http://sci2s.ugr.es/keel/pdf/algorithm/articulo/dietterich1998.pdf).\n",
    "\n",
    "Under H0, the two algorithms should have the same error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "def mcnemar(x, y):\n",
    "    n1 = np.sum(x < y)\n",
    "    n2 = np.sum(x > y)\n",
    "    stat = (np.abs(n1-n2)-1)**2 / (n1+n2)\n",
    "    df = 1\n",
    "    pval = chi2.sf(stat,1)\n",
    "    return stat, pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Compare classifiers\n",
    "\n",
    "- Choose the decision tree max_depth in [2..6], criterion in ['entropy', 'gini'] and splitter in ['best', 'random']. What are the best parameters? Print out all grid scores to sanity check the selection. Is there a unique best set of parameters?\n",
    "- Use `np.array` create `l_yn` and `t_yn` arrays showing respectively for logistic regression and decision tree whether each test instance is predicted correctly (`1`) or incorrectly (`0`). Are the classifiers significantly different at p<=0.05 according to McNemar's test?(use the logistic regression code from previous week)\n",
    "- Which classifier is significantly better at p<=0.05 using paired t-test?(use f-score measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Para for accuracy {'criterion': 'entropy', 'max_depth': 2, 'splitter': 'best'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        34\n",
      "           1       0.92      1.00      0.96        33\n",
      "           2       1.00      0.91      0.95        33\n",
      "\n",
      "    accuracy                           0.97       100\n",
      "   macro avg       0.97      0.97      0.97       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "para = [\n",
    "    {'max_depth': [2, 3, 4, 5, 6], \n",
    "     'criterion': ['entropy', 'gini'],\n",
    "     'splitter': ['best', 'random']\n",
    "    }\n",
    "]\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(tree, para, cv = 10, scoring = 'accuracy')\n",
    "clf.fit(X_train, Y_train) \n",
    "print('Best Para for accuracy', clf.best_params_)\n",
    "print(classification_report(Y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "P value is  1.0\n",
      "stat 0.0\n"
     ]
    }
   ],
   "source": [
    "# compare log and decision tree\n",
    "# output prediction based on two algorithms\n",
    "# output the p value using McNemar test\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "# fit logistic regression \n",
    "logit = LogisticRegression()\n",
    "_ = logit.fit(X_train, Y_train) \n",
    "l_pred = logit.predict(X_test) \n",
    "l_yn = [int(p==t) for p, t in zip(l_pred, Y_test)]\n",
    "\n",
    "# fit tree\n",
    "tree = DecisionTreeClassifier()\n",
    "_ = tree.fit(X_train, Y_train) \n",
    "t_pred = tree.predict(X_test) \n",
    "t_yn = [int(p==t) for p, t in zip(t_pred, Y_test)]\n",
    "\n",
    "stat, pval = mcnemar(l_yn, t_yn)\n",
    "\n",
    "print(l_yn, t_yn)\n",
    "\n",
    "print('P value is ', pval) \n",
    "print('stat', stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qz/gxp970q51gg714xs883jd5640000gn/T/ipykernel_20280/1858711835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ml_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_iter'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "l_predict = []\n",
    "t_predict = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size = 0.3, random_state = 0)\n",
    "    logit = LogisticRegression(max_iter = 500, random_state = 0) \n",
    "    _ = logit.fit(X_train, Y_train) \n",
    "    l_predict.append(f1_score(Y_test, logit.predict(X_test), average = 'macro'))\n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_iter = 500, random_state = 0) \n",
    "    _ = tree.fit(X_train, Y_train) \n",
    "    t_predict.append(f1_score(Y_test, tree.predict(X_test), average = 'macro'))\n",
    "    \n",
    "print(l_predict)\n",
    "print(t_predict) \n",
    "\n",
    "import scipy.stats as stats\n",
    "stats.ttest_rel(l_predict, t_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bea42e1b0e07028483ba0ff26b9b4dc4fa162e9d0ccb6b0507d54b9d42d30653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
